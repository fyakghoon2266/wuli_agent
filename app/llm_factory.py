# app/llm_factory.py

from langchain_openai import ChatOpenAI, AzureChatOpenAI
from langchain_aws import ChatBedrock
# æ³¨æ„ï¼šå¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ–°ç‰ˆ langchainï¼Œå¯èƒ½éœ€è¦æ”¹ç‚º from langchain.agents import ...
from langchain_classic.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# å¼•å…¥é…ç½®èˆ‡æ–‡æ¡ˆ
from app.config import settings
from app.prompts import SYSTEM_PROMPT

# å¼•å…¥ RAG åˆå§‹åŒ–å‡½å¼
from app.rag.retriever import init_rag

# å¼•å…¥æ‹†åˆ†å¾Œçš„å·¥å…· (è«‹ç¢ºä¿é€™äº›æª”æ¡ˆå·²å»ºç«‹)
from app.tools.ops import search_error_cards, search_litellm_logs_admin, search_litellm_logs_user
from app.tools.communication import send_email_to_engineer
from app.tools.security import verify_prompt_with_guardrails
from app.tools.search import get_search_tool
from app.tools.git_ops import propose_new_error_card
from app.tools.incident import log_incident_for_weekly_report
from app.tools.selfie import send_wuli_photo
from app.tools.jira_ops import report_issue_to_jira
from app.tools.lifecycle import check_model_eol

def build_llm():
    """
    æ ¹æ“š app/config.py çš„è¨­å®šï¼Œå»ºç«‹å°æ‡‰çš„ LLM å¯¦é«”ã€‚
    """
    provider = settings.LLM_PROVIDER
    
    if provider == "azure":
        # æª¢æŸ¥å¿…è¦åƒæ•¸
        if not (settings.AZURE_OPENAI_ENDPOINT and settings.AZURE_OPENAI_API_KEY and settings.AZURE_OPENAI_DEPLOYMENT):
             raise RuntimeError("LLM_PROVIDER=azureï¼Œä½† AZURE_OPENAI_* ç›¸é—œè¨­å®šä¸å®Œæ•´ã€‚")
             
        return AzureChatOpenAI(
            azure_endpoint=settings.AZURE_OPENAI_ENDPOINT,
            api_key=settings.AZURE_OPENAI_API_KEY,
            api_version=settings.AZURE_OPENAI_API_VERSION,
            azure_deployment=settings.AZURE_OPENAI_DEPLOYMENT,
            timeout=settings.TIMEOUT_SECONDS,
            temperature=0.2,
            streaming=True
        )

    elif provider == "bedrock":
        return ChatBedrock(
        model_id=settings.BEDROCK_MODEL_ID,  # æˆ–è€…ç”¨ haiku / opus
        region_name=settings.AWS_REGION,  # æˆ–æ˜¯ä½ æ¨¡å‹é–‹é€šçš„å€åŸŸï¼Œå¦‚ us-west-2
        model_kwargs={
            "temperature": 0.2,
        }
    )

    else: # é è¨­ç‚º OpenAI
        if not settings.OPENAI_API_KEY:
            raise RuntimeError("LLM_PROVIDER=openaiï¼Œä½† OPENAI_API_KEY æœªè¨­å®šã€‚")

        return ChatOpenAI(
            api_key=settings.OPENAI_API_KEY,
            model=settings.OPENAI_MODEL,
            timeout=settings.TIMEOUT_SECONDS,
            temperature=0.2,
            streaming=True
        )

def build_agent_executor(is_admin: bool = False):
    """
    çµ„è£ LLMã€Tools èˆ‡ Promptï¼Œå»ºç«‹ Agent åŸ·è¡Œå™¨ã€‚
    """

    """
    æ ¹æ“šæ˜¯å¦ç‚ºç®¡ç†å“¡ï¼Œå›å‚³ä¸åŒæ¬Šé™çš„ Agent
    """
    
    if is_admin:
        log_tool = search_litellm_logs_admin
    else:
        log_tool = search_litellm_logs_user
    
    # ğŸ”¥ å¼·åˆ¶å°‡å·¥å…·åç¨±çµ±ä¸€ï¼Œé€™æ¨£ System Prompt ä¸éœ€è¦ç‚ºäº†ä¸åŒäººå¯«å…©å¥—
    log_tool.name = "search_litellm_logs"

    # 2. å®šç¾©åŸºç¤å·¥å…·
    base_tools = [
        search_error_cards,            
        log_tool,                      # <--- é€™è£¡æ”¾å‹•æ…‹æ±ºå®šçš„å·¥å…·
        get_search_tool,               
        verify_prompt_with_guardrails, 
        send_wuli_photo,               
        check_model_eol,              
        send_email_to_engineer,        
    ]

    # 2. å®šç¾©ç®¡ç†å“¡å·¥å…· (åªæœ‰ Admin èƒ½ç”¨ï¼šå¯«å…¥ã€ç™¼ä¿¡ã€é–‹ç¥¨)
    admin_tools = [
        propose_new_error_card,        # æ–°å¢éŒ¯èª¤çŸ¥è­˜åº«
        log_incident_for_weekly_report,# å¯«é€±å ±
        report_issue_to_jira           # é–‹ Jira å–®
    ]

    # 3. æ ¹æ“šæ¬Šé™çµ„åˆå·¥å…·ç®±
    if is_admin:
        print("ğŸ›¡ï¸  å•Ÿç”¨ Admin æ¨¡å¼ï¼šæˆæ¬Šæ‰€æœ‰é«˜é¢¨éšªå·¥å…·")
        tools = base_tools + admin_tools
    else:
        print("ğŸ‘¤ å•Ÿç”¨ User æ¨¡å¼ï¼šåƒ…æˆæ¬Šå”¯è®€/æŸ¥è©¢å·¥å…·")
        tools = base_tools
    # 1. åˆå§‹åŒ– RAG (è¼‰å…¥ ChromaDB)
    # æ”¾åœ¨é€™è£¡çš„å¥½è™•æ˜¯ï¼šåªæœ‰åœ¨ Agent çœŸæ­£è¦è¢«å»ºç«‹æ™‚ï¼Œæ‰æœƒå»è®€å– Vector DBï¼ŒåŠ å¿« import é€Ÿåº¦
    init_rag() 

    # 2. å»ºç«‹ LLM
    llm = build_llm()

    # 4. è¨­å®š Prompt Template
    # ä½¿ç”¨ ChatPromptTemplate è®“çµæ§‹æ›´æ¸…æ™°
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT), 
        MessagesPlaceholder(variable_name="chat_history"),
        # ("human", "{input}"),
        MessagesPlaceholder(variable_name="user_message"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # 5. å»ºç«‹ Agent
    # create_tool_calling_agent æ˜¯ LangChain é‡å°æ”¯æ´ Function Calling æ¨¡å‹ (GPT/Claude) çš„æœ€ä½³å¯¦ä½œ
    agent = create_tool_calling_agent(llm, tools, prompt)

    # 6. å›å‚³åŸ·è¡Œå™¨
    return AgentExecutor(agent=agent, tools=tools, verbose=True)


class AgentSingleton:
    """
    å–®ä¾‹æ¨¡å¼ç®¡ç†å™¨ (Singleton Pattern)
    ç¢ºä¿æ•´å€‹æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸä¸­ï¼ŒAgentExecutor åªæœƒè¢«åˆå§‹åŒ–ä¸€æ¬¡ã€‚
    é¿å…é‡è¤‡é€£ç·šè³‡æ–™åº«æˆ–é‡è¤‡è¼‰å…¥ RAG æ¨¡å‹ã€‚
    """
    _instance = None
    
    @classmethod
    def get_executor(cls):
        if cls._instance is None:
            print("ğŸ¤– åˆå§‹åŒ– Wuli Agent ...")
            cls._instance = build_agent_executor()
            print("âœ… Wuli Agent å°±ç·’ï¼")
        return cls._instance